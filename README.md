# machineLearning
用于记录机器学习相关算法及算法处理中遇见的问题
一、KNN算法实现及注意点：
  1）原理：邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的
  是每个样本都可以用它最接近的k个邻居来代表。
  2）核心思想：
  kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。
  3）优缺点：
  优点：
    1.简单，易于理解，易于实现，无需估计参数，无需训练；
    2. 适合对稀有事件进行分类；
    3.特别适合于多分类问题(multi-modal,对象具有多个类别标签)，
     kNN比SVM的表现要好。
    缺点：
    1.该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 
    2.该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。
    3.可理解性差，无法给出像决策树那样的规则。
    4.KNN算法中K值的选取具有一定的随意性，可能会出现程序返回值变化的情况，即多次运行程序结果不一致。
二、决策树
  1、基本概念
	决策树(Decision Tree）是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价
项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。Entropy = 系统的凌乱程度，使用算法ID3, C4.5和C5.0生成树算法使用熵。这一度量是基于信息学理论中熵的概念。
	决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。
分类树（决策树）是一种十分常用的分类方法。他是一种监管学习，所谓监管学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。
2、决策树算法
	对于决策树来说，主要有两种算法：ID3算法和C4.5算法。
2.1、ID3算法介绍
	ID3算法是决策树的一种，它是基于奥卡姆剃刀原理的，即用尽量用较少的东西做更多的事。ID3算法， 即Iterative 
Dichotomiser 3，迭代二叉树3代，是Ross Quinlan发明的一种决策树算法，这个算法的基础就是上面提到的奥卡姆剃刀原理，越是小型的决策树越优于大的决策树，尽管如此，也不总是生成最小的树型结构，而是一个启发式算法。
 
	在信息论中，期望信息越小，那么信息增益就越大，从而纯度就越高。ID3算法的核心思想就是以信息增益来度量属性的选
择，选择分裂后信息增益最大的属性进行分裂。该算法采用自顶向下的贪婪搜索遍历可能的决策空间。
2.2、信息熵与信息增益
	在信息增益中，重要性的衡量标准就是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。在认识信息增
益之前，先来看看信息熵的定义。
	熵这个概念最早起源于物理学，在物理学中是用来度量一个热力学系统的无序程度，而在信息学里面，熵是对不确定性的度
量。在1948年，香农引入了信息熵，将其定义为离散随机事件出现的概率，一个系统越是有序，信息熵就越低，反之一个系统越是混乱，它的信息熵就越高。所以信息熵可以被认为是系统有序化程度的一个度量。
	假如一个随机变量X的取值为X={x1,x2,…,xn},每一种取到的概率分别是{p1,p2,…,pn},那么X的熵定义为：
H(X)=−∑24_(i=1)^n▒〖p_i 〖log⁡〗_2 p_i 〗
	意思是一个变量的变化情况越多，那么它携带的信息量就越大。
	对于分类系统来说，类别C是变量，它的取值是C1,C2,C3,…,Cn，而每一个类别出现的概率分别是P(C1),P(C2),…,P(Cn)，而这里的n
就是类别的总数，此时分类系统的熵就可以表示如下：
H(C)=−∑24_(i=1)^n▒〖p_Ci 〖log⁡〗_2 p_Ci 〗
	信息增益是针对一个一个特征而言的，就是看一个特征，系统有它和没有它时的信息量各是多少，两者的差值就是这个特征给
系统带来的信息量，即信息增益。
	以天气预报的例子来说明。下面是描述天气数据表，学习目标是play或者not play

  
