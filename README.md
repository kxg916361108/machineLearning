# machineLearning
用于记录机器学习相关算法及算法处理中遇见的问题
一、KNN算法实现及注意点：
  1）原理：邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的
  是每个样本都可以用它最接近的k个邻居来代表。
  2）核心思想：
  kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。
  3）优缺点：
  优点：
    1.简单，易于理解，易于实现，无需估计参数，无需训练；
    2. 适合对稀有事件进行分类；
    3.特别适合于多分类问题(multi-modal,对象具有多个类别标签)，
     kNN比SVM的表现要好。
    缺点：
    1.该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 
    2.该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。
    3.可理解性差，无法给出像决策树那样的规则。
    4.KNN算法中K值的选取具有一定的随意性，可能会出现程序返回值变化的情况，即多次运行程序结果不一致。
    
   
  
